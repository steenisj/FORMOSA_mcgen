# Master milliQan MC generation tutorial
The full milliQan simulation pipeline involves numerous steps, pulling from multiple repositories written by multiple people. The initial event generation is handled by this repository, the propagation engine is in [MilliqanSim](https://github.com/claudiocc1/MilliqanSim), Geant4 simulation and ntuplization is in [milliQanDemoSim](https://github.com/milliQan-sw/milliQanDemoSim), and pulse injection is handled by macros in [milliqanOffline](https://github.com/milliQan-sw/milliqanOffline). Various scripts for analysis, plotting, and deriving calibrations/systematics are scattered across these repositories.

Here I (=Bennett) have tried to organize everything into a single place, to provide some kind of instruction for anyone in the future trying to run through this whole process.

[Note: A good high-level overview of the whole process used for the demonstrator paper can be found in [these slides](https://docs.google.com/presentation/d/1RQ2l8dykOk_f2HA-a-QmwXzpqHG1SnlHV4S8BNC7Lc4/edit?usp=sharing). A description
of all of the theory/experiment comparisons used to derive cross sections and uncertainties are in [these slides](https://docs.google.com/presentation/d/1fFwFP4A6epERyUvavHiA5V4gAMegDhOSLu6k6_2aDrE/edit?usp=sharing).]

[Also note: some of the merging/skimming scripts depend on my copyTree.py tool. That can be [found here](https://github.com/cmstas/Software/tree/master/scripts). You should put this somewhere and then add that directory to your PATH]

## Event Generation

Generation of initial events (mCPs or beam muons) is handled by this [milliq_mcgen](https://github.com/claudiocc1/milliq_mcgen) repository. First, we consider mCPs. Cross sections and p<sub>T</sub> distributions for all of the various production modes come from different places. All of these should already be in place in a fresh clone of the repository, and you can proceed straight to the generation, but I list them here for reference.
* Light meson production is done with pythia. All use the pythia8 Monash2013 tune, except for &phi;s which use the pythia6 DW tune. Instructions/scripts for running pythia and generating the p<sub>T</sub> distributions can be found in the [mesonPt](../mesonPt) directory. The output is a set of histograms with units of "particles per minbias event per 50 MeV bin" (for the eta range |&eta;|<2). So, the total cross section for a given particle is the sum of all bin contents ("particles per minbias event") times the MinBias cross section. The histograms used in the demonstrator paper are in [pt_dists.root](../mesonPt/pt_dists.root).
* J/&psi; and &psi;' production come from theory. We have merged together separate low-p<sub>T</sub> and high-p<sub>T</sub> files; see [oniaDirect/CMS-13-TeV/theory/psiLowPt](../oniaDirect/CMS-13-TeV/theory/psiLowPt) for the scripts and merged files (merged_*.root). These files contain d&sigma;/dp<sub>T</sub> histograms, for |&eta;|<1.2.
* Upsilon production comes from experiment (high-p<sub>T</sub> from 13 TeV ATLAS data, low-p<sub>T</sub> from 7 TeV CMS data). See [oniaDirect/upsilon](../oniaDirect/upsilon) for scripts and merged files (ups*_combined.root). The histograms again contain d&sigma;/dp<sub>T</sub> for |&eta;|<1.2, but this time the branching ratios to &mu;&mu; are folded in; this is divided out in the generation stage).
* Drell-Yan is taken from MadGraph. See the [madgraphDY](../madgraphDY) directory for instructions. For the purposes of the demonstrator paper, since DY is subdominant everywhere important, it was sufficient to run jobs locally overnight with the `parallel` utility. For higher stats/more mass points, grid submission will probably need to be implemented. **Update:** I (=Bennett)ve now added grid submission capabilities, see `madgraphDY/batchsubmit` for instructions.

For all non-DY modes, we need to convert the differential cross section distributions into ntuples of generated mCPs. This is done with the tools in the [decayMCP](../decayMCP) directory. `DecayGen` is a class that interfaces with the various histograms described above, extracts total cross sections/BRs, and performs the decays. It shouldn't need to be touched unless you change the format of cross section histograms (or want to change hardcoded MinBias xsec at the top). The ntuples are generated with `runDecays`, following the instructions in [decayMCP](../decayMCP). It produces an `MCPTree`, with branches described in the README. Note that if the milliQan location is changed, you will have to edit `MCP_[ETA,PHI][MAX,MIN]` in `runDecays.cc`.

For a full production, it is easiest to submit jobs to the grid. Scripts and instructions are in [decayMCP/localbatch](../decayMCP/localbatch). These scripts automatically adjust the number of events in each production mode so that event weights are ~constant (i.e. # of events is proportional to xsec*BR). If this is done correctly, you should have a hadoop directory structured as `<mass_dir>/<mode_dir>/output_*.root` with MCPTrees in the ROOT files.

**Misc:** C++ files containing the functions used to compute branching ratios and do the four-vector decay math are in [utils](../utils). Python versions (not really used for anything any more) are in [scripts](../scripts). These shouldn't need to be touched unless you find a bug (hopefully not, since we will have published with that bug....). Scripts to generate a ROOT file with mCP cross sections and turn these into the [xsec plot used in the paper](../scripts/plot-xsecs/mcp-xsec.pdf) are in [scripts/plot-xsecs](../scripts/plot-xsecs).

**Muons:** The equivalent event generation for muons is in the [muons](../muons) directory. Cross section data for the various modes is in [muons/data](../muons/data). The QCD modes come from pythia, using the same tools as described above. Batch submission tools are in [muons/localbatch](../muons/localbatch). Note that it is currently set up pretty inefficiently, as it generates the same number of events for each mode even though the QCD modes have much higher cross sections than the W/Z modes. For the future someone should fix this to generate in rough proportion to the total cross sections, like I did for mCPs.

## Propagation

Once we have the generated events, we need to propagate them to the milliQan detector. The propagation engine that drives this is in the [MilliqanSim](https://github.com/claudiocc1/MilliqanSim) repository; details can be found there. The present repository contains a wrapper around this in the [propagate](../propagate) directory. This takes the output ntuples from the previous step, propagates them to the milliQan detector, and adds relevant branches to the tree. General instructions are in the README there.

`config.py` contains the configurations that specify the location (eta/phi/R) of detector, the amount of rock, etc. The `det_width` and `det_height` parameters control the size of the rectangle in which to save particles. If a trajectory does not hit this rectangle, the event is thrown away. The `MilliqanDetector` object in `run_sim.py` controls the bar/slab arrangement (this doesn't really matter if you're just going to run through Geant; I've used this in the past as a "fast simulation" to get quick numbers for rates of signal going through bars, not accounting for detection efficiency).

The [propagate/batchsubmit](../propagate/batchsubmit) directory has scripts for batch submission of the entire mass/charge grid. `makeConfigFile.py` will generate a condor submission file for each (m,q) point, organized into directories. You will have to edit some parameters at the top: `ntuple_tag` is the tag of the gen ntuples you made in the previous step, `sim_tag` is whatever you want to tag this propagation, `config` should be "MQ" for default MilliQan, `dens_mult` scales all material densities by a fixed number (for deriving a systematic), and `save_dist` is the distance in meters before the milliqan detector face you want to save mCP trajectories. You'll also have to edit indir/outdirs because my username is hardcoded. Once you run this, you can run `. submit_all.sh configs/<ntuple_tag>_<sim_tag>` to submit all config files with condor. I probably should have set this up with [metis](https://github.com/aminnj/ProjectMetis), but I didn't. So for a very hacky way of monitoring jobs/automatically resubmitting failures, open a screen session and run `python checkAllConfig.py configs/<ntuple_tag>_<sim_tag>`.

For muons, you can run a similar procedure with `makeMuonConfigFile.py`.

The [propagate/scripts](../propagate/scripts) directory has a mess of scripts for handling/analyzing this propagation output. `. merge.sh <ntuple_tag> <sim_tag>` will merge the files (edit outdir since I am hardcoded). `. skim.sh <ntuple_tag> <sim_tag>` can apply a skim to these, which I used to restrict to a smaller rectangle before passing to Geant. (actually, these scripts just print a list of commands that need to be run. I then run them with the [parallel](https://www.gnu.org/software/parallel/) utility, which can be downloaded locally and put in your PATH. Run with something like `parallel --nice 19 --jobs 15 --bar --shuf --joblog joblog.txt < commands.txt`, where `commands.txt` is the file with one command per line you want to run.)

`root2text.py` will turn the skimmed ROOT files into the text files that are currently required by the Geant sim.

Running `get_rates.py` then `plot_rates.py` will make useful plots like [these](http://uaf-8.t2.ucsd.edu/~bjmarsh/milliqan/milliq_mcgen/plots/rate/v8_v1_save2m/) that plot the rates of incidence, either just hitting the defined rectangle ("rate") or going through three bars in a line ("line_rate"). `plot_kinematics.py` will make plots of various quantities for mCPs that make it to the detector, like [these](http://uaf-8.t2.ucsd.edu/~claudiocc1/milliqan/milliq_mcgen/plots/kinematics/v6_v1/q_0p1/). `plot_etaphi.py` will dump plots of the initial eta/phi/pt of mCPs that hit detector, along with vertical dashed lines that indicate the generation bounds (like [this](http://uaf-8.t2.ucsd.edu/~bjmarsh/milliqan/milliq_mcgen/plots/etaphi/v7/m_1p0/q_0p05/)). **You should make sure that the cuts don't cut off the distributions, or else you are excluding some of the rate**. I (Bennett) checked this for demonstrator, but if detector location/size is changed, you should check that they are still good (the place to change the actual cuts is in `runDecays.cc` from the generation step).

`muon_plots.py` will make useful plots of muons that make it to the detector, like [this](http://uaf-8.t2.ucsd.edu/~bemarsh/milliqan/milliq_mcgen/plots/muons/v5_v6_save2m/).

The `muangles` directory has scripts to do the rough check of muon angular distribution, like [this](http://uaf-8.t2.ucsd.edu/~bemarsh/milliqan/geant_sim/muangles/test/geantsim_run3_franny_data/top.pdf).

## Geant4 simulation

The Geant4 simulation is contained in the [milliQanDemoSim](https://github.com/milliQan-sw/milliQanDemoSim) repository. It was developed and maintained by Ryan Schmitz, so he is the one to ask questions on how to run. It takes as input text files of propagated mCPs/muons, generated with the `root2text.py` script described above.

## Geant ntuplization

Geant produces raw ROOT files with low-level information on individual tracks and PMT hits. This is a bit hard to analyze, so I've created an ntupler that takes the raw Geant output and produces ntuples with higher level information, aggregated by channel. It is located in the [slim_ntupler](https://github.com/milliQan-sw/milliQanDemoSim/tree/master/slim_ntupler) directory of milliQanDemoSim. General instructions on compiling and running, as well as a description of the output format, can be found in the README there.

I'll discuss here various constants/calibrations used in the ntupler that might need to be changed in the future.
* `rotAngle, xOffset, zOffset`: constants necessary to convert from Geant coordinates to the sim coordinates I use here (same as from the propagation stage mentioned above. x and y axes are parallel to and centered on the detector face. +z points along detector in the direction of a beam-based particle). I stole these from the detector definition in the Geant sim source
* `channel_map`: convert from Geant PMT numbers to milliQan channel numbers
* `time_calibs`: subtract this number from the time of PEs in each channel, so that a speed-of-light beam-based particle will deposit hits at the same time in every channel. I just derived these manually by looking at timing distributions for each channel.
* `cosmic_paths`: a muon must hit all of these channels to count as a vertical cosmic (top panel and three bars in a vertical line)
* `pmt_positions`: the x,y,z positions in Geant coordinates of each channel's PMT. Not really important, I only added because we were interested in correlations between PE timing and total path length. Derived manually by looking at distributions of PE position for each PMT
* `pmt_calibrations`: if `APPLY_CALIB` (at top) is set to `true`, only accept this fraction of PEs in each channel (done randomly, by generating a random number for each PE and accepting with this probability).

All of the branches with info about the material in which hits originated (`chan_fracElScint`, `chan_fracElAl`, etc) involve tracing tracks through the parent-child relationships back to the source. The various functions for doing this are in `utils/TrackFinding.[cc,h]`.

There are scripts for batch submission in the `metis` directory. There are separate submission scripts for beam muons, cosmic muons, and mCPs. Once this is done, you can merge them with `merge_*.sh`. Similar to above, I run these with the `parallel` utility. I run the mixing over these merged files, so they need to be copied somewhere on hadoop.

**Looper:** I have a looper set up that runs over the ntupler output and makes histograms used to make nice plots [like this](http://uaf-8.t2.ucsd.edu/~bemarsh/milliqan/geant_sim/beammuons_run3/inclusive/nPE/), located in `slim_ntupler/looper`. Sorry, this has gotten quite convoluted as I've added more and more histograms to it. Anyway, if you can manage to get it to run you can dump plots with the `slim_ntupler/looper/make_plots.py` script. I also have a script to make data/MC plots [like this](http://uaf-8.t2.ucsd.edu/~bemarsh/milliqan/geant_sim/data_comp/run3_pmtcalib/nonneighbBars/) (though I didn't run over data, that was mostly from Franny) in `slim_ntupler/datacomp`.

**Event display:** I've made a jsroot-based event display to visualize all of the tracks in an event ([see here for some examples](http://uaf-10.t2.ucsd.edu/~bemarsh/dump/jsroot/index.htm?file=inputs/mq_events.root)). First we need to extract all of the tracks in an event. There is a tool to do this in `slim_ntupler/printTracks` that runs over raw Geant output. Within an output file of the ntupler, there are branches `fileID` and `orig_event` that give the index of the original Geant output file and the event number within that file. Once you know the file and event number you want to inspect, run `./printTracks /path/to/geant/output/file.root event_number ch1 [ch2 ch3 ...]`, where the last arguments are a list of channels whose PE hits you want to inspect. You can just pass -1 as ch1 if you want to do all channels. This will dump a bunch of probably-not-comprehensible-except-to-bennett information to stdin. Redirect to a text file (possibly located in `../scripts/events`) with whatever name you want. Now go to `slim_ntupler/scripts`, and you can run `python event_display.py events/*` (or change the argument to whatever text files you want to draw). Note you will have to change the path near the top of the script to point to your local clone of the MilliqanSim repository, which controls a lot of the geometry. If this works, you will have a `mq_events.root` file that you can upload to a server where you have installed [jsroot](https://root.cern.ch/js/).

**Calibration toys:** For deriving the NPE calibration systematic, I have a C++ program to generate "calibration toys" by randomly varying PMT efficiencies and re-computing SR yields. This is in `slim_ntupler/calibration_toys`. The uncertainty model is defined in the main program `generate_toys.cc`. It takes a while to generate 1000 toys, so there are batch submission scripts in the `metis` directory. This makes use of `genNPE <> recoNPE` templates (to convert from genNPE counts to recoNPE counts, taking into account reconstruction inefficiencies). These are generated from pulse-injected ntuples with `make_recogen_templates.py`.

## Pulse injection

Pulse injection is the trickiest step, since it involves the tree-making code in [milliqanOffline](https://github.com/milliQan-sw/milliqanOffline) that really wasn't designed for batch submission.  The latest code is stored locally in a milliqan user area (`ssh milliqan@[uafino,cms2].physics.ucsb.edu`, usual password), at `/net/cms26/cms26r0/milliqan/milliqanOffline`. `mix_events.C` (compile with `compileMix.sh`) performs the injection and creates DRS-like output files with raw waveforms. Then `make_tree.C` makes the final tree in the same way as for actual data (Matthew is the main author for both of these, so ask him with questions). These can be run locally, but there are many hard-coded paths/files that make them not work for batch submission.

For now, I've taken a very hacky approach by duplicating them into the `batchsubmit_mixing` directory, along with all of their file dependencies, and editing them to refer to relative paths instead of absolute ones. This is obviously not very sustainable since every time make_tree is updated, it needs to be updated in two locations. Ideally the workflow should be improved so this is not needed, but I never got around to it.

Anyway, within the `batchsubmit_mixing` directory you can compile the programs with `compileMix.sh` and `compile.sh`, then run `createTar.sh` to bundle everything into a tarball. This tarball can be copied to the UAF and used for batch submission.

I have metis scripts for batch submission in [milliQanDemoSim/slim_ntupler/batchsubmit_mixing](https://github.com/milliQan-sw/milliQanDemoSim/tree/master/slim_ntupler/batchsubmit_mixing). There are separate submission files for muons and mCPs. Note that the injection uses zero-bias events, so I had to manually copy a number of zero-bias data trees to hadoop on the UAF (located at `/hadoop/cms/store/user/bemarsh/milliqan/zero_bias/`). The `condor_exe.sh` explicitly points to these when running the mixing. The executable is set to only copy the final analysis tree, but if you want the intermediate file with DRS-like raw waveforms (e.g. for making event displays), you can uncomment the final block in `condor_exe.sh` (note that these files are very big, so do this carefully).
